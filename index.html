<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="" />
    <meta name="author" content="" />
    <title>Fighting Biases in AI</title>
    <!-- Favicon-->
    <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
    <!-- Font Awesome icons (free version)-->
    <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
    <link href="https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
        type="text/css" />
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="css/styles.css" rel="stylesheet" />
</head>

<body id="page-top">
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg bg-secondary text-uppercase fixed-top" id="mainNav">
        <div class="container">
            <a class="navbar-brand js-scroll-trigger" href="#page-top"> Home </a>
            <button
                class="navbar-toggler navbar-toggler-right text-uppercase font-weight-bold bg-primary text-white rounded"
                type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive"
                aria-expanded="false" aria-label="Toggle navigation">
                Menu
                <i class="fas fa-bars"></i>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger"
                            href="#problems">problems</a></li>
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger"
                            href="#affect">effects</a></li>
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger"
                            href="#solutions">solutions</a></li>
                    <li class="nav-item mx-0 mx-lg-1"><a class="nav-link py-3 px-0 px-lg-3 rounded js-scroll-trigger"
                            data-toggle="modal" data-target="#ref" target="_blank">References </a></li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Masthead-->
    <header class="masthead bg-primary text-white text-center">
        <div class="container d-flex align-items-center flex-column">
            <img class=" masthead-avatar mb-2" src="assets/img/aiJudge.png" alt="" />
            <h1 class="masthead-heading text-uppercase ">Fighting biases in AI</h1>
            <p class="masthead-subheading font-weight-light"> <i>For the people working in the field of AI.</i></p>
            <p class="lead text-left">
                <style>
                    p {
                        text-indent: 50px;
                    }
                </style>
                Artificial Intelligence is built based on large data sets and pattern finding
                algorithms that find similarities between training data points and patterns to come to a
                conclusion for a particular issue. These data sets are the building blocks of machine learning
                and this process can easily introduce bias in AI algorithms. The first step toward fighting bias
                in AI is to acknowledge its existence and work to make sure that the AI you build does not
                further discriminatory bias. However, the realm of artificial intelligence is quite complex. Bias can
                never be completely removed due to AI’s use of commonalities to make predictions. How do we accurately
                and
                respectfully come to algorithmic conclusions about people without over generalizing everyone?
                The world is vast and full of intersectional people and cultures, yet AI needs to put everyone
                into categories in order to function as an algorithm. In this site we will explore examples of biasing
                problems that arise in AI, how these problems
                affect people as well as why they are critical to discuss, and solutions to mitigate these
                biases that you can apply in the industry.
            </p>
        </div>

        </div>
    </header>

    <!-- Problem Section-->
    <section class="page-section portfolio" id="problems">
        <div class="container">
            <!-- Problems Section Heading-->
            <h2 class="page-section-heading text-center text-uppercase text-secondary">Problems</h2>
            <!-- Icon Divider-->
            <div class="divider-custom">
                <div class="divider-custom-line"></div>
                <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                <div class="divider-custom-line"></div>
            </div>

            <!-- Problems Grid Items-->
            <div class="row">
                <!-- Problems Item 1-->
                <div class="col-md-6 col-lg-6 mb-5">
                    <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal1">
                        <div
                            class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                            <div class="portfolio-item-caption-content text-center text-white"><i
                                    class="fas fa-plus fa-3x"></i></div>
                        </div>
                        <img class="img-fluid" src="assets/img/problem/p1.png" alt="" />
                        <h3 class="font-weight-bolder text-center text-uppercase text-secondary">Data unrepresentative
                            of the reality</h3>
                    </div>
                </div>
                <!-- Problems Item 2-->
                <div class="col-md-6 col-lg-6 mb-5">
                    <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal2">
                        <div
                            class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                            <div class="portfolio-item-caption-content text-center text-white"><i
                                    class="fas fa-plus fa-3x"></i></div>
                        </div>
                        <img class="img-fluid" src="assets/img/problem/p2.png" alt="" />
                        <h3 class="font-weight-bolder text-center text-uppercase text-secondary">Data with historical
                            prejudices/ favoritism</h3>
                    </div>
                </div>
            </div>
            <div class="row">
                <!-- Problems Item 3-->
                <div class="col-md-6 col-lg-6 mb-5 mb-lg-0">
                    <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal3">
                        <div
                            class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                            <div class="portfolio-item-caption-content text-center text-white"><i
                                    class="fas fa-plus fa-3x"></i></div>
                        </div>
                        <img class="img-fluid" src="assets/img/problem/p3.png" alt="" />
                        <h3 class="font-weight-bolder text-center text-uppercase text-secondary">Intentional bias during
                            goal setting</h3>
                    </div>
                </div>
                <!-- Problems Item 4-->
                <div class="col-md-6 col-lg-6 mb-5 mb-md-0">
                    <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal4">
                        <div
                            class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                            <div class="portfolio-item-caption-content text-center text-white"><i
                                    class="fas fa-plus fa-3x"></i></div>
                        </div>
                        <img class="img-fluid" src="assets/img/problem/p4.png" alt="" />
                        <h3 class="font-weight-bolder text-center text-uppercase text-secondary">Interaction with the AI
                            model</h3>
                    </div>
                </div>
            </div>

    </section>

    <!-- Affect Section-->
    <section class="page-section portfolio " id="affect">
        <div class="container">
            <!-- Problems Section Heading-->
            <h2 class="page-section-heading text-center text-uppercase text-secondary">HOW DOES AI DISCRIMINATE/SHOW
                BIAS?</h2>
            <!-- Icon Divider-->
            <div class="divider-custom">
                <div class="divider-custom-line"></div>
                <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                <div class="divider-custom-line"></div>
            </div>

            <!-- affect Grid Items-->
            <div class="row">
                <!-- affect Item 1-->
                <div class="col-md-6 col-lg-6 ">
                    <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal5">
                        <div
                            class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                            <div class="portfolio-item-caption-content text-center text-white"><i
                                    class="fas fa-plus fa-3x"></i></div>
                        </div>
                        <img class="img-fluid" src="assets/img/affect/a1.png" alt="" />
                        <h3 class="font-weight-bolder text-center text-uppercase text-secondary">HOW AI AFFECTS PEOPLE
                        </h3>
                    </div>
                </div>
                <!-- Problems Item 2-->
                <div class="col-md-6 col-lg-6 ">
                    <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal6">
                        <div
                            class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                            <div class="portfolio-item-caption-content text-center text-white"><i
                                    class="fas fa-plus fa-3x"></i></div>
                        </div>
                        <img class="img-fluid" src="assets/img/affect/a2.png" alt="" />
                        <h3 class="font-weight-bolder text-center text-uppercase text-secondary">HOW AI WORSENS
                            DISCRIMINATION</h3>
                    </div>
                </div>
            </div>
    </section>


    <!-- Solution Section-->
    <section class="page-section portfolio" id="solutions">
        <div class="container">
            <!-- Solution Section Heading-->
            <h2 class="page-section-heading text-center text-uppercase text-secondary mb-0">Solutions</h2>
            <!-- Icon Divider-->
            <div class="divider-custom">
                <div class="divider-custom-line"></div>
                <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                <div class="divider-custom-line"></div>
            </div>
            <!-- Solution Grid Items-->
            <div class="row">
                <!-- Solution Item 1-->
                <div class="col-md-6 col-lg-6">
                    <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal7">
                        <div
                            class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                            <div class="portfolio-item-caption-content text-center text-white"><i
                                    class="fas fa-plus fa-3x"></i></div>
                        </div>
                        <img class="img-fluid" src="assets/img/solutions/s1.png" alt="" />
                        <h3 class="font-weight-bolder text-center text-uppercase text-secondary">Pre Processing Data
                        </h3>
                    </div>
                </div>
                <!-- Solution Item 2-->
                <div class="col-md-6 col-lg-6">
                    <div class="portfolio-item mx-auto" data-toggle="modal" data-target="#portfolioModal8">
                        <div
                            class="portfolio-item-caption d-flex align-items-center justify-content-center h-100 w-100">
                            <div class="portfolio-item-caption-content text-center text-white"><i
                                    class="fas fa-plus fa-3x"></i></div>
                        </div>
                        <img class="img-fluid" src="assets/img/solutions/s2.png" alt="" />
                        <h3 class="font-weight-bolder text-center text-uppercase text-secondary">POST PROCESSING AND
                            ALGORITHMIC ANALYSIS</h3>
                    </div>
                </div>

            </div>
        </div>
    </section>


    <!-- Scroll to Top Button (Only visible on small and extra-small screen sizes)-->
    <div class="scroll-to-top d-lg-none position-fixed">
        <a class="js-scroll-trigger d-block text-center text-white rounded" href="#page-top"><i
                class="fa fa-chevron-up"></i></a>
    </div>
    <!-- whats going on-->
    <!-- Problems/Solution Modals-->
    <!-- Problems/Solution Modal 1-->
    <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog"
        aria-labelledby="portfolioModal1Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <!-- Problems/Solution Modal - Title-->
                                <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0"
                                    id="portfolioModal1Label">Data unrepresentative of the reality</h2>
                                <!-- Icon Divider-->
                                <div class="divider-custom">
                                    <div class="divider-custom-line"></div>
                                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                    <div class="divider-custom-line"></div>
                                </div>
                                <!-- Problems/Solution Modal - Image-->
                                <img class="img-fluid rounded" src="assets/img/problem/p1.png" alt="" />
                                <p class="font-weight-light"> <i>Image Credit:
                                        https://lionbridge.ai/articles/7-types-of-data-bias-in-machine-learning/</i></p>
                                <!-- Problems/Solution Modal - Text-->
                                <p class="lead text-left">This type of bias occurs when your dataset sample does not
                                    reflect the true reality of the overall population. It can be possible that you
                                    believe you have enough data for every group but sometimes some group(s) can be
                                    represented less than others in your dataset. So, the model that is training using
                                    these kinds of biases dataset is prone to be inaccurate for the group that is
                                    unrepresentative of the overall population.
                                    <br>
                                    This is exactly what happened to the facial recognition software that is trained
                                    using more photos of light skin tone faces than dark skin tone faces. As a result,
                                    the AI model that is designed to detect human face has a really hard time detecting
                                    dark skin tone face.
                                </p>
                                <div class="button-row">
                                    <div class="button-col">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://www.media.mit.edu/projects/actionable-auditing-coordinated-bias-disclosure-study/overview/');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>
                                    <div class="button-col">
                                        <button class="btn btn-primary" data-dismiss="modal">
                                            <i class="fas fa-times fa-fw"></i>
                                            Close
                                        </button>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Problems/Solution Modal 2-->
    <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog"
        aria-labelledby="portfolioModal2Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <!-- Problems/Solution Modal - Title-->
                                <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0"
                                    id="portfolioModal2Label">Data with historical prejudices/ favoritism</h2>
                                <!-- Icon Divider-->
                                <div class="divider-custom">
                                    <div class="divider-custom-line"></div>
                                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                    <div class="divider-custom-line"></div>
                                </div>
                                <!-- Problems/Solution Modal - Image-->
                                <img class="img-fluid rounded" src="assets/img/problem/p2.png" alt="" />
                                <p class="font-weight-light"> <i>Image Credit:
                                        https://www.theladders.com/career-advice/amazon-reportedly-scraps-ai-recruiting-tool-biased-against-women</i>
                                </p>
                                <!-- Problems/Solution Modal - Text-->
                                <p class="lead text-left">This type of bias occurs when your dataset already contains
                                    historical prejudices and favoritism to one group than the other during the data
                                    gathering process. For example, Amazon used their historical data from the last 10
                                    years to train their AI model to review their job applicants’ resumes and predict if
                                    the applicant is a match for the job or not.
                                    <br>
                                    Little did they know, their AI recruiting model was not predicting applicants
                                    fairly, but it showed bias toward women applicants. Their historical dataset
                                    contained favoritism towards male applicants since historically tech industry was
                                    dominated by men. So, their AI model trained on this biased dataset learned to
                                    reward the resume of male applicants and penalized the resume of the female
                                    applicants.
                                </p>

                                <div class="button-row">
                                    <div class="button-col">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>
                                    <div class="button-col">
                                        <button class="btn btn-primary" data-dismiss="modal">
                                            <i class="fas fa-times fa-fw"></i>
                                            Close
                                        </button>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Problems/Solution Modal 3-->
    <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog"
        aria-labelledby="portfolioModal3Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <!-- Problems/Solution Modal - Title-->
                                <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0"
                                    id="portfolioModal3Label">Intentional bias during goal setting</h2>
                                <!-- Icon Divider-->
                                <div class="divider-custom">
                                    <div class="divider-custom-line"></div>
                                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                    <div class="divider-custom-line"></div>
                                </div>
                                <!-- Problems/Solution Modal - Image-->
                                <img class="img-fluid rounded" src="assets/img/problem/p3.png" alt="" />
                                <p class="font-weight-light"> <i>Image Credit:
                                        https://www.tctmd.com/news/race-and-gender-bias-may-sway-decisions-advanced-hf-care</i>
                                </p>
                                <!-- Problems/Solution Modal - Text-->
                                <p class="lead text-left">Before you even start to gather data and think about training
                                    your AI model, you have to decide what is your business problem and how the AI model
                                    is going to solve that problem. Bias can be lured into your model during this goal
                                    setting phase. This is exactly what happened to Facebook when they were trying to
                                    target their user with personalized housing advertisements.
                                    <br>
                                    Facebook intentionally let their AI model target their user based on race, gender,
                                    class, and religion. As a result of this mistake, Facebook AI ads model was pushing
                                    jobs like nursing to women and jobs like janitors to minority men.
                                </p>

                                <div class="button-row">
                                    <div class="button-col">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://news.northeastern.edu/2019/12/18/facebooks-ad-delivery-system-still-discriminates-by-race-gender-age-y/');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>
                                    <div class="button-col">
                                        <button class="btn btn-primary" data-dismiss="modal">
                                            <i class="fas fa-times fa-fw"></i>
                                            Close
                                        </button>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Problems/Solution Modal 4-->
    <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" role="dialog"
        aria-labelledby="portfolioModal4Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <!-- Problems/Solution Modal - Title-->
                                <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0"
                                    id="portfolioModal4Label">Interaction with the AI model</h2>
                                <!-- Icon Divider-->
                                <div class="divider-custom">
                                    <div class="divider-custom-line"></div>
                                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                    <div class="divider-custom-line"></div>
                                </div>
                                <!-- Problems/Solution Modal - Image-->
                                <img class="img-fluid rounded" src="assets/img/problem/p4.png" alt="" />
                                <p class="font-weight-light"> <i>Image Credit:
                                        https://www.cio.com/article/3403668/top-6-chatbot-building-platforms.html</i>
                                </p>
                                <!-- Problems/Solution Modal - Text-->
                                <p class="lead text-left">Your AI model can be biased even after you fairly gather your
                                    dataset, accurately and fairly trained to AI model. Yes, your AI models can be
                                    biased after the deployment. This is what happened to Microsoft’s AI chatbot when
                                    they let their chatbot to interact with Twitter. Their goal was to make their
                                    chatbot smarter by having people on Twitter to interact with their chatbot. But
                                    their project went sideways when the chatbot started sharing anti-Semitic and racist
                                    tweets. Under 15 hours, this AI chatbot learned to be racist and sexist through the
                                    interaction with other Twitter users.
                                </p>


                                <div class="button-row">
                                    <div class="button-col">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>
                                    <div class="button-col">
                                        <button class="btn btn-primary" data-dismiss="modal">
                                            <i class="fas fa-times fa-fw"></i>
                                            Close
                                        </button>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <!-- Problems/Solution Modal 5-->
    <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" role="dialog"
        aria-labelledby="portfolioModal5Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <!-- Problems/Solution Modal - Title-->
                                <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0"
                                    id="portfolioModal5Label">HOW AI AFFECTS PEOPLE</h2>
                                <!-- Icon Divider-->
                                <div class="divider-custom">
                                    <div class="divider-custom-line"></div>
                                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                    <div class="divider-custom-line"></div>
                                </div>
                                <!-- Problems/Solution Modal - Image-->
                                <img class="img-fluid rounded" src="assets/img/affect/a1.png" alt="" />
                                <p class="font-weight-light"> <i>Image Credit:
                                        https://techxplore.com/news/2019-07-bias-ai.html</i>
                                </p>
                                <!-- Problems/Solution Modal - Text-->
                                <p class="lead text-left">The following information below is reported from AI Now
                                    Institute:<br>
                                    The diversity and inclusion data AI companies release to the public is a partial
                                    view, and often contains flaws. One researcher found that Google’s diversity report
                                    was designed to artificially inflate the numbers of women and people of color
                                    employed by the company by only accounting for 80% of the company’s full-time
                                    workforce. The data presented in these reports not only gives a limited view but
                                    also has historically excluded figures that would provide key insights into gender
                                    and race-based discrimination in tech companies. Quote from AI Now Institute:“For
                                    example, analysis of data from the 2010-12 American Community Survey by the American
                                    Institute for Economic Research found that there are substantial pay disparities
                                    among high tech workers: on average, female software developers of color earn less
                                    than white, black, and Asian men, as well as white women. Latina software developers
                                    earned as much as 20% less annually than white male software developers.” The reason
                                    why discrimination and inequity in the workplace harms minoritized communities is
                                    because of their exclusion from resources and opportunities needed to prosper. The
                                    patterns of discrimination and exclusion resonate well beyond the workplace into the
                                    bigger world. Industrial AI systems, at an ever-increasing rate, are playing a role
                                    in our social and political institutions, including in education, healthcare,
                                    hiring, and criminal justice.As a result we need to consider the connection between
                                    the crisis that exists in workplace diversity and the issues with bias and
                                    discrimination in AI systems.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Fairness, accountability, and transparency research is playing a prominent role in
                                    tracking down the scale and scope of gendered and racialized discrimination in AI
                                    systems.Quote from AI Now Institute: “For example, a recent study found that
                                    mechanisms in Facebook’s ad delivery systems led users to be shown ads for housing
                                    and employment in a discriminatory manner. With the same targeted audience, and
                                    without the advertisers intending or being aware, ads are delivered in a manner that
                                    aligns with gender and racial stereotypes: ads for jobs in the lumber industry were
                                    disproportionately shown to white male users, while ads for cashier positions at
                                    supermarkets were shown to female users and ads for taxi drivers to black users. By
                                    experimenting with Google’s search engine results, Safiya Noble demonstrated that
                                    Google search results retrieve highly sexualized imagery for searches on terms like
                                    “black girls” and “latina girls”. In a landmark study, Latanya Sweeney found that
                                    two search engines disproportionately serve ads for arrest records against searches
                                    for racially associated names.”This study is crucial to evaluate because it
                                    demonstrates how the algorithm implicitly shows racist stereotypes through the job
                                    advertisments were for BIPOC. Another quote from AI Institute Now, “A 2019 study
                                    found significant racial bias in a widely used commercial algorithm used to
                                    determine whether patients will be enrolled in ‘care management’ programs that
                                    allocate considerable additional resources: white patients were far more likely to
                                    be enrolled in the program and to benefit from its resources than black patients in
                                    a comparable state of health. The forensic examination of individual systems for
                                    bias and discrimination is an important area of research, and more studies like
                                    these are sorely needed.” This study is important because it shows how algorithms
                                    demonstrate racism through being selective with the demographics they choose to
                                    recommend proper resources to and BIPOC who suffer from degrading stereotypes will
                                    be left out and not receive the proper care they need and deserve.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    There are other aspects of discrimination present in AI systems that must be
                                    evaluated. We must have AI systems broaden their lens of bias since the way bias is
                                    perceived in the system only is included in a technical matter. As a result, there
                                    are many forms of discriminations that exist that might come from AI systems that
                                    would not fit in the technical definitions of these systems. Disadvantages are
                                    typically seen in an economic sense and the social and political consequences of AI
                                    systems harmfully representing and interpreting certain groups of people receive
                                    less attention. Quote from AI Institute Now: “Rather than solely focusing on
                                    improving existing datasets or individual algorithms, future work could also more
                                    thoroughly account for how societal discrimination surfaces in data provenance,
                                    examining the history and process of dataset construction, and considering how
                                    cultural norms and stereotypes were numerated and represented at the time of data
                                    creation. For example, according to Han and Jain, while the popular Labeled Faces in
                                    the Wild (LFW) dataset contains over 15,000 images of faces, only 7% are images of
                                    black people.” This quote is important because with LFW’s representation of human
                                    faces, we are able to see a social hierarchy they are trying to portray through
                                    visual media.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Also, a lot of studies on bias and discrimination function on a single factor
                                    instead of examining multiple identity categories intersecting with each other.
                                    Quote from AI Institute Now: “This is likely to produce what Erica Joy Baker,a
                                    Senior Engineering Manager at Patreon, calls colorless diversity: without
                                    acknowledging the ways in which different forms of oppression intersect, diversity
                                    efforts that target women without acknowledging the role of race and other forms of
                                    identity (let alone the broader spectrum of gendered identity) will implicitly
                                    privilege white women.” Another example of this can be seen in a 2018 Gender Shades
                                    paper by Joy Buolamwini and Timnit Gebru which examined three commercial facial
                                    recognition systems that included the ability to classify faces by gender and
                                    encountered how they had a tendency to display higher error rates for darker-skinned
                                    women(predominantly Black women) than for any other group, with the lowest error
                                    rates for light skinned men(predominantly white men).Gender and racial bias has been
                                    connected to the creation of the datasets used to train these systems, which, like
                                    Labeled Faces in the Wild, were overwhelmingly composed of lighter-skinned
                                    male-looking subjects.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Furthermore, a lot of studies examining bias in gender tend to show a binary view of
                                    what gender looks like. A lot of studies don’t take into account the experiences of
                                    trans men and women and non-binary people who have an identity outside of male or
                                    female. Recognizing the fluidity of gender in bias studies is not only needed for
                                    accuracy, it will also prevent cases that will result in a systematic erasure of
                                    trans people and their experiences.

                                </p>


                                <div class="button-row">
                                    <div class="button-col">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://ainowinstitute.org/discriminatingsystems.pdf');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>
                                    <div class="button-col">
                                        <button class="btn btn-primary" data-dismiss="modal">
                                            <i class="fas fa-times fa-fw"></i>
                                            Close
                                        </button>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Problems/Solution Modal 6-->
    <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" role="dialog"
        aria-labelledby="portfolioModal6Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <!-- Problems/Solution Modal - Title-->
                                <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0"
                                    id="portfolioModal6Label">HOW AI WORSENS DISCRIMINATION</h2>
                                <!-- Icon Divider-->
                                <div class="divider-custom">
                                    <div class="divider-custom-line"></div>
                                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                    <div class="divider-custom-line"></div>
                                </div>
                                <!-- Problems/Solution Modal - Image-->
                                <img class="img-fluid rounded" src="assets/img/affect/a2.png" alt="" />
                                <p class="font-weight-light"> <i>Image Credit:
                                        https://towardsdatascience.com/bias-and-algorithmic-fairness-10f0805edc2b</i>
                                </p>
                                <!-- Problems/Solution Modal - Text-->
                                <p class="lead text-left">
                                    The following information below is reported from the book
                                    Carceral Capitalism by Jackie Wang:<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Jackie Wang in her book “Carceral Capitalism” begins talking about how she was
                                    googling a law-enforcement start-up company called PredPol. As she did so,
                                    advertisements from that company popped up on her Twitter feed and found it strange
                                    considering she only searched up the PredPol to discover predictive analytics used
                                    in certain law enforcement practices.The advertisement that popped up on her feed
                                    talked about how the company is not using personal data to help law enforcement
                                    create safer communities even though it is clear that they are clearly using
                                    personal data from Wang’s google searches in their advertising.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Ever since the 19th century there has been a data-driven approach used to understand
                                    crime which as Wang stated, “has been used to perpetuate institutionalized anti
                                    blackness and legitimize policing.” This goes to show how it's no surprise data
                                    would be used to perpetuate a racist narrative that Black people and POC commit more
                                    crimes when in reality they are unfairly targeted at disproportionately higher rates
                                    despite not committing crimes at higher rates. Law enforcement sectors receive lots
                                    of money from tech industries to develop predictive policing technology. Wang states
                                    how much law enforcement receives by saying, “By late 2013, PredPol alone received
                                    $1.3 million in seed funding by Silicon Valley Investors.”<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    The foundation of PredPol and other predictive policing technologies can be traced
                                    to criminologist George Kelling who was affiliated with advocating for the use of
                                    statistical analysis to more "effectively" distribute law enforcement resources
                                    beginning in the 1980s.In the mid-1990s, ComStat was introduced into the New York
                                    Police Department, which encouraged officers to decide which areas to police based
                                    on statistical analysis rather than intuition. Ever since that time, more than 150
                                    police departments nation-wide that have adopted policing software and equipment
                                    that allows for statistical analysis. The developers of PredPol were concerned with
                                    not only creating a tool that would make law enforcement more efficient, but also
                                    constructing a brand that would increase the interest of the media. <br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    In the past several years the public perception towards the police has shifted,
                                    especially considering social movements being a factor that influenced the
                                    perception. Wang reports a finding from the National Institute of Justice that
                                    demonstrates "Research consistently shows that minorities are more likely than
                                    whites to view law enforcement with suspicion and distrust."Because there are
                                    critics of police that associate them with excessive use of force, racial profiling
                                    and the power to decide who lives and who dies, certain law enforcement have been
                                    focused on rebranding the police in a way that foregrounds statistical impersonality
                                    and symbolically removes the agency of individual officers is a clever way to cast
                                    police activity as neutral, unbiased, and rational. Doing this removes police from
                                    taking any responsibility. Even though their actions might demonstrate their
                                    implicit bias against certain demographics, using science and data as a counter
                                    argument will distance them from their actions and unjustifiably release them from
                                    guilt.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Crime data gathered by the police to determine where officers should go sends police
                                    to patrol the poor neighborhoods they have historically patrolled when they were
                                    guided by their intuitions and biases. The crime data gathered disproportionately
                                    affects BIPOC since a majority of poor neighborhoods have BIPOC living there so
                                    therefore they are the ones unfairly targeted when police make their rounds in
                                    neighborhoods that are patrolled on a frequent basis. People that work for law
                                    enforcement in the technology side cater to people's desire for law and order to
                                    manipulate it by having companies that use algorithmic policing practices to prevent
                                    crime and terrorism at home and broad. Other examples such as catastrophes, war, and
                                    crime epidemics also may further deepen our collective desire for security. This
                                    shows how assuming there is always a potential threat somewhere only adds to our
                                    individual and collective anxieties.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Certain data and numbers we find don't capture the humanity we are trying to
                                    maintain when talking about certain patterns and algorithms that target people.
                                    Furthermore, the automated data used can be too general and we must approach data we
                                    get from people in a complex and individualized lens because the data you have on
                                    people cannot and will not reflect everyone accurately. Even though algorithmic
                                    policing has the goal to reduce crime, if there were no social threats to manage,
                                    these companies would be out of business. A UCLA professor named Brantingham
                                    emphasizes in his promotion of PredPol that by reducing human actors to their innate
                                    instincts and applying complex mathematical models to track the behavior of these
                                    urban "hunter-gathers,", his predictive policing model attempts to create "order"
                                    out of the seeming disorder of human behavior. This model perpetuates racial
                                    profiling and discrimination by minimizing human behavior simply to innate
                                    instincts, this technology could be biased against certain minority groups who have
                                    historically been seen as inherent threats to society.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Santa Cruz, Ca was one of the first cities to use PredPol where they used 11 years
                                    of local crime data to make predictions on where crime would occur. Specifically,
                                    police departments that use PredPol, officers are given printouts of jurisdiction
                                    maps that are covered with red square boxes that indicate where crime is supposed to
                                    occur throughout the day with the hope of either catching a criminal or deter crime.
                                    Even though this program is intended to eliminate racial profiling, it is still
                                    perpetuating racial profiling in a less explicit way. These red square boxes are
                                    based on a probability of crime which is mostly of lower socioeconomic status. In
                                    addition, past data isn't reliable and is too general. Wang poses a question on when
                                    officers enter through one of the red boxes, is there a coincidence they could
                                    stumble upon crime taking place. She also poses the question on whether will people
                                    who pass through these temporary crime zones while they are being patrolled by
                                    officers automatically be perceived as suspicious. These predictive statistics can
                                    be so vague and confusing because we don't know the factors that led to these
                                    statistics so using predictive statistics will never be an effective way of
                                    understanding how or why crime happens in certain places. This shows PredPol is
                                    affecting the way we move in certain spaces based on algorithmic policing and this
                                    disproportionately instills fear into the lives of BIPOC folks.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Measuring the effectiveness of predictive policing methods is difficult and with
                                    that comes a risk of falsely associating "positive" law enforcement outcomes with
                                    the use of predictive policing software such as PredPol. As a result, some innocent
                                    people can be targeted falsely or sometimes police don't catch the actual criminal.
                                    Police should not fully rely on these technologies and instead have other tools
                                    readily available. PredPol perceives the increase in arrests in designated areas as
                                    an indicator of success, while the other way around suggests that a decrease in
                                    crime is proof that the software works. However in modern day a lot of us would
                                    associate the increase of arrests in designated areas as a failure of the system and
                                    decrease in arrests as a sign that the system is being confronted. PredPol offered
                                    50 percent discounts on the software given to police departments that agreed to
                                    participate in pilot cities to try out the programs. This software when implemented
                                    has the power to do two major things: legitimize historically racist practices and
                                    remove responsibility from the officers themselves so that way police officers can
                                    absolve themselves from any guilt of racially profiling minoritized racial
                                    communities.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Wang brings up an important point by mentioning how crime has never been a neutral
                                    category. What gets counted as a crime, who gets labeled as a criminal and which
                                    areas are policed have historically been racialized. Moreover, even though the
                                    datasets serve as a function to make predictions, police should not rely on this
                                    model when addressing crime in the city they work in. This can leave certain police
                                    feeling entitled to let data misrepresent certain marginalized communities and not
                                    check the disproportionate numbers being presented. For example, if someone commits
                                    a crime in an area that is not heavily checked by the police like a higher income
                                    area and/or a predominantly white area, it will fail to generate any sufficient
                                    data. Which shows how you can't rely on using spatial algorithmic policing. While
                                    methods developed by PredPol themselves are not explicitly racialized, they show
                                    implicit racialization in terms of geography being a proxy for race. <br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Domestic police departments have received $34 billion in federal grants from the
                                    Department of Homeland Security after 9/11. Repressive policing attempts to respond
                                    to events that already occurred meanwhile with algorithmic policing, it attempts to
                                    maintain law and order by actively preventing crime. However, as seen through all
                                    these studies, algorithmic policing has led to increased threats towards
                                    disproportionately targeted minority communities rather than achieving safety.

                                </p>

                                <button class="btn btn-primary" data-dismiss="modal">
                                    <i class="fas fa-times fa-fw"></i>
                                    Close
                                </button>


                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <!-- Problems/Solution Modal 7-->
    <div class="portfolio-modal modal fade" id="portfolioModal7" tabindex="-1" role="dialog"
        aria-labelledby="portfolioModal7Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <!-- Problems/Solution Modal - Title-->
                                <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0"
                                    id="portfolioModal7Label">Pre Processing Data</h2>
                                <!-- Icon Divider-->
                                <div class="divider-custom">
                                    <div class="divider-custom-line"></div>
                                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                    <div class="divider-custom-line"></div>
                                </div>
                                <!-- Problems/Solution Modal - Image-->
                                <img class="img-fluid rounded" src="assets/img/solutions/s1.png" alt="" />
                                <p class="font-weight-light"> <i>Image Credit:
                                        https://medium.com/@mauriziosantamicone/https-medium-com-mauriziosantamicone-is-artificial-intelligence-racist-66ea8f67c7de
                                    </i>
                                </p>
                                <!-- Problems/Solution Modal - Text-->
                                <p class="lead text-left">
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    The first step to reducing bias is to take note of what your AI is intending to
                                    accomplish and what data is relevant to achieve this goal. Algorithms use data sets
                                    that have many factors in them, sometimes in the millions, in order to determine
                                    results like bank loans, job recruiting, bail bonds, and facial recognition. In the
                                    case of AI used to determine the top candidates for a job, there are many factors
                                    that can be relevant such as level of education, years of experience, letters of
                                    recommendation, and more. However factors such as gender and race are sometimes used
                                    as factors in data sets and leads to these machines learning that people of a
                                    certain gender and race, most typically white males, are more likely to be qualified
                                    for a job. This is due to systemic and historical issues that have made white males
                                    more likely to be successful compared to others which in return impacts data sets to
                                    select them more than others. In job candidate algorithms, race and gender should
                                    not be used as factors as they are not relevant to one’s ability to perform a job
                                    successfully and the use of this will continue to support the glass ceiling on non
                                    white non males in the workforce. As we can see, it is very important to thoroughly
                                    analyze and understand the problem that your AI is trying to solve and then
                                    determine which factors in your data set are relevant to utilize. Keeping in mind
                                    the ramifications of continuing to perpetuate historical inequities is important and
                                    this can be curtailed by having only relevant factors that do not further
                                    discrimination. Too many irrelevant factors in data sets leads to incorrect
                                    conclusions as algorithms can draw parallels between unrelated factors leading to
                                    increased bias.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Another way to reduce bias in data set preprocessing is to use datasets that are
                                    very inclusive of people of varying racial and gender backgrounds and are
                                    representative of the population. A major flaw in machine learning is the use of
                                    large non representative data sets. Typically these data sets are packed with data
                                    from white males and have considerably less non white and non male data. This means
                                    that artificial intelligence is predominantly built on white male data for accuracy
                                    on white males. This is highly flawed and does not hold up well when used across
                                    various racial and gender identities that end up varying drastically from the AI
                                    model that was not trained on people like them. Since society does not consist
                                    solely of this white male class of people, data sets must be more inclusive and
                                    representative of the population in order to be more accurate.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    When data collection is not representative, issues arise such as simply not being
                                    able to detect or compute results for a POC. One example of this is seen with facial
                                    recognition failing to detect darker toned faces while white men perform greatly. As
                                    Joy Buolamwini states in her Ted Talk on fighting bias in algorithms, facial
                                    recognition is built from a large data set of primarily white men with very low
                                    inclusion of others which makes it very difficult for her Black female face to be
                                    recognized. However, when she places a white mask over her face, her face is
                                    instantly detected showing the algorithmic bias within many facial detection
                                    programs. This clearly shows the detrimental and non inclusive effects of a non
                                    representative data set. Data sets must have more inclusion otherwise white males
                                    become the standard and anyone who differs from this ends up as a small majority of
                                    the data set. This can sometimes be seen more so as noise in the data, rather than
                                    valid data, which does not lend itself to being a significant part of the trained AI
                                    algorithm.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Since there is low data training that is inclusive of people of various racial and
                                    gender backgrounds, when these algorithms are built and tested on new people of
                                    similar backgrounds there is a strong algorithmic tendency to treat these people
                                    exactly like that subset of data. While a particular data set may have millions of
                                    data on white men, people of other ethnic and gender backgrounds may only have data
                                    in the thousands. This algorithmically reduces anyone who is tested on this
                                    algorithm to have the same tendencies as this small subset of people from the
                                    training set that had the same background. This easily continues to perpetuate
                                    stereotypes and discriminate since there is likely historical inequities in this
                                    group which is further magnified when the test group is algorithmically determined
                                    to behave in the same way. Within a racial group there is still variation along many
                                    factors, some being wealth, age, height and weight, and more. But having low
                                    representation in the training set reduces people to the small range given in the
                                    training. White males on the other hand have a large range of people with differing
                                    features in these same factors. So accuracy on white males is much higher as there
                                    is a wide range the AI is trained on unlike for other ethnic groups that are reduced
                                    to being one and the same due to minimal representation in training sets. In order
                                    to combat this, training data sets must be much more inclusive of the ethnic and
                                    gender backgrounds of the population for higher accuracy and less discriminatory
                                    biases in the algorithms.
                                </p>

                                <div class="button-row">
                                    <div class="button-col4">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://www.youtube.com/watch?v=UG_X_7g63rY');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>
                                    <div class="button-col4">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>
                                    <div class="button-col4">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://techcrunch.com/2018/11/06/3-ways-to-avoid-bias-in-machine-learning/');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>

                                    <div class="button-col4">
                                        <button class="btn btn-primary" data-dismiss="modal">
                                            <i class="fas fa-times fa-fw"></i>
                                            Close
                                        </button>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
    <!-- Problems/Solution Modal 8-->
    <div class="portfolio-modal modal fade" id="portfolioModal8" tabindex="-1" role="dialog"
        aria-labelledby="portfolioModal8Label" aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body text-center">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <!-- Problems/Solution Modal - Title-->
                                <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0"
                                    id="portfolioModal8Label">Post Processing and Algorithmic Analysis</h2>
                                <!-- Icon Divider-->
                                <div class="divider-custom">
                                    <div class="divider-custom-line"></div>
                                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                    <div class="divider-custom-line"></div>
                                </div>
                                <!-- Problems/Solution Modal - Image-->
                                <img class="img-fluid rounded" src="assets/img/solutions/s2.png" alt="" />
                                <p class="font-weight-light"> <i>Image Credit:
                                        https://www.obviously.ai/post/5-organizations-increasing-diversity-in-ai
                                    </i>
                                </p>
                                <!-- Problems/Solution Modal - Text-->
                                <p class="lead text-left">
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    While efforts to mitigate bias begin in the pre processing of
                                    data, post processing analysis is also critical once the AI program has been
                                    trained. Live
                                    testing the program is critical to check for bias and issues in the algorithms. The
                                    testing must be done on a large group of people of various racial and gender
                                    backgrounds and then validated for accuracy. If there is discrimination or low
                                    accuracy amongst certain groups, this must be noted and analyzed. Analysis is one of
                                    the most critical tools for fighting biases in AI. Ask yourself: why is this
                                    algorithm disproportionately affecting these groups? Is it based on low
                                    representation in the training data? Is it due to historical inequities and
                                    prejudice? Is it due to a non-inclusive algorithm? Are these results from the
                                    algorithm furthering racial and gender discrimination? These questions will lead to
                                    productive discussions that inspire solutions to help mitigate bias.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    This process will also be assisted by having a diverse machine learning team. People
                                    of diverse backgrounds raise different questions and interact with AI in differing
                                    ways that help account for potential issues that can then be solved by the team.
                                    Diversity in technology fields helps promote inclusivity in AI and reduce bias.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Data weighting is another tool that can help with data bias especially when there is
                                    low amounts of data for certain groups of people. Weights can be applied to the data
                                    points from these smaller racial and gender groups in order to boost their
                                    importance in training. However, this must be done very carefully as artificial
                                    boosting can also create more bias by picking up on noise in data and magnifying
                                    that. If done lightly and properly, data weighting can help reduce some bias however
                                    caution must be taken to prevent these groups from again falling trap to being
                                    treated exactly as their similar counterparts in the training data.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Cross validation can be used to verify the accuracy of an algorithm with information
                                    that can potentially be biasing. This process involves training the algorithm on all
                                    data sets apart from the potentially biasing factor. Then both the original
                                    algorithm and the algorithm without the potential bias should be validated on an
                                    inclusive test model for accuracy. If there is significant difference between them
                                    this could mean that that factor is a cause of discrimination and thus should be
                                    analyzed further to determine whether this is a harmful factor or an important one
                                    that does not harm groups of people and solely increases algorithmic accuracy.<br>
                                    <style>
                                        p {
                                            text-indent: 50px;
                                        }
                                    </style>
                                    Counterfactual modeling can also be used to decrease bias. A bank noticed that women
                                    were seen as credit risks much more frequently than men despite there not being a
                                    factual basis for this. They utilized IBM’s Watson OpenScale to manage their AI
                                    system and used counterfactual modeling where they ran their credit risk algorithm
                                    on women to see if they were a loan risk. If the woman was seen as a risk they would
                                    rerun the program but change her gender to male. If she was no longer a risk when
                                    labeled as male in the algorithm, the bank determined that it was an algorithmic
                                    bias and she was not actually a risk. The bank utilized this to help mitigate bias
                                    and prevent wrongful denials of loans and high interest rates to people who did not
                                    deserve this for the simple fact that they are female.
                                </p>

                                <div class="button-row">
                                    <div class="button-col4">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://www.wsj.com/articles/how-to-make-artificial-intelligence-less-biased-11604415654');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>
                                    <div class="button-col4">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://hbr.org/2020/11/a-simple-tactic-that-could-help-reduce-bias-in-ai');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>
                                    <div class="button-col4">
                                        <button class="btn btn-primary"
                                            onClick="window.open('https://appen.com/blog/how-to-reduce-bias-in-ai/');">
                                            <i class="fas fa-link"></i>
                                            Link
                                        </button>
                                    </div>

                                    <div class="button-col4">
                                        <button class="btn btn-primary" data-dismiss="modal">
                                            <i class="fas fa-times fa-fw"></i>
                                            Close
                                        </button>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <!-- ref-->
    <div class="portfolio-modal modal fade" id="ref" tabindex="-1" role="dialog" aria-labelledby="refLabel"
        aria-hidden="true">
        <div class="modal-dialog modal-xl" role="document">
            <div class="modal-content">
                <button class="close" type="button" data-dismiss="modal" aria-label="Close">
                    <span aria-hidden="true"><i class="fas fa-times"></i></span>
                </button>
                <div class="modal-body ">
                    <div class="container">
                        <div class="row justify-content-center">
                            <div class="col-lg-8">
                                <!-- Modal - Title-->
                                <h2 class="portfolio-modal-title text-secondary text-uppercase mb-0vc text-center"
                                    id="refLabel">References</h2>
                                <!-- Icon Divider-->
                                <div class="divider-custom">
                                    <div class="divider-custom-line"></div>
                                    <div class="divider-custom-icon"><i class="fas fa-star"></i></div>
                                    <div class="divider-custom-line"></div>
                                </div>
                                <!--  Modal - Text-->
                                <ul>
                                    <li>“Argentina: Child Suspects’ Private Data Published Online.” Human Rights Watch,
                                        9 Oct. 2020,
                                        https://www.hrw.org/news/2020/10/09/argentina-child-suspects-private-data-published-online.
                                    </li>
                                    <li>Buolamwini, Joy. “How I'm Fighting Bias in Algorithms.” YouTube, YouTube, 29
                                        Mar. 2017, www.youtube.com/watch?v=UG_X_7g63rY.</li>
                                    <li>Callahan, Molly. Facebook’s Ad Delivery System Still Discriminates by Race,
                                        Gender, Age. 18 Dec. 2019,
                                        https://news.northeastern.edu/2019/12/18/facebooks-ad-delivery-system-still-discriminates-by-race-gender-age-y/.
                                    </li>
                                    <li>Dastin, Jeffrey. “Amazon Scraps Secret AI Recruiting Tool That Showed Bias
                                        against Women.” Reuters,
                                        https://www.reuters.com/article/us-amazon-com-jobs-automation-insight-idUSKCN1MK08G.
                                        Accessed 19 Dec. 2020.</li>
                                    <li>“How to Reduce Bias in AI with a Focus on Training Data.” Appen, 6 Aug. 2020,
                                        appen.com/blog/how-to-reduce-bias-in-ai/.</li>
                                    <li>Lynch, Vince. “Three Ways to Avoid Bias in Machine Learning.” TechCrunch, 6 Nov.
                                        2018, techcrunch.com/2018/11/06/3-ways-to-avoid-bias-in-machine-learning/.</li>
                                    <li>“Project Overview ‹ Actionable Auditing: Coordinated Bias Disclosure Study – MIT
                                        Media Lab.” MIT Media Lab,
                                        https://www.media.mit.edu/projects/actionable-auditing-coordinated-bias-disclosure-study/overview/.
                                        Accessed 19 Dec. 2020.</li>
                                    <li>Simonite, Tom. “When It Comes to Gorillas, Google Photos Remains Blind.” WIRED,
                                        19 Dec. 2020,
                                        https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/.
                                    </li>
                                    <li>“This Is a Story About Nerds and Cops.” Carceral Capitalism, by Jackie Wang,
                                        Semiotext(e), 2018.
                                        https://app.perusall.com/courses/cgs-108-ltcs-108-gender-race-and-artificial-int-mendelsohn-fa20/jackie-wang-this-is-a-story-about-nerds-and-cops-_-predpol-and-algorithmic-policing-from-carceral-capitalism-2018.
                                    </li>
                                    <li>Totty, Michael. “How to Make Artificial Intelligence Less Biased.” The Wall
                                        Street Journal, Dow Jones & Company, 3 Nov. 2020,
                                        www.wsj.com/articles/how-to-make-artificial-intelligence-less-biased-11604415654.
                                    </li>
                                    <li>Turner-Lee, Nicol, et al. “Algorithmic Bias Detection and Mitigation: Best
                                        Practices and Policies to Reduce Consumer Harms.” Brookings, Brookings, 25 Oct.
                                        2019,
                                        www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/.
                                    </li>
                                    <li>Uzzi, Brian. “A Simple Tactic That Could Help Reduce Bias in AI.” Harvard
                                        Business Review, 4 Nov. 2020,
                                        hbr.org/2020/11/a-simple-tactic-that-could-help-reduce-bias-in-ai. </li>
                                    <li>Vincent, James. “Twitter Taught Microsoft’s Friendly AI Chatbot to Be a Racist
                                        Asshole in Less than a Day.” The Verge, 24 Mar. 2016,
                                        https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist.</li>
                                    <li>West, Sarah Myers, et al. “Discriminating Systems:Gender, Race, and Power in
                                        AI.” AI Now Institute, AI Now , Apr. 2019,
                                        ainowinstitute.org/discriminatingsystems.pdf. </li>

                                </ul>
                                <div class="text-center">
                                    <button class="btn btn-primary" data-dismiss="modal">
                                        <i class="fas fa-times fa-fw"></i>
                                        Close
                                    </button>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>


    <!-- Bootstrap core JS-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.bundle.min.js"></script>
    <!-- Third party plugin JS-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
    <!-- Contact form JS-->
    <script src="assets/mail/jqBootstrapValidation.js"></script>
    <script src="assets/mail/contact_me.js"></script>
    <!-- Core theme JS-->
    <script src="js/scripts.js"></script>
</body>

</html>